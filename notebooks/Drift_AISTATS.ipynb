{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Iz3CK87RQFz"
   },
   "source": [
    "Basic Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1677190127664,
     "user": {
      "displayName": "Pranjal Awasthi",
      "userId": "08866363694134506097"
     },
     "user_tz": 480
    },
    "id": "oQo1ju15NCti"
   },
   "outputs": [],
   "source": [
    "\"\"\"Basic Python Imports\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi1sVYcgRLTV"
   },
   "source": [
    "Discrepancy computation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1677190130725,
     "user": {
      "displayName": "Pranjal Awasthi",
      "userId": "08866363694134506097"
     },
     "user_tz": 480
    },
    "id": "B6zDuLQCNb-3"
   },
   "outputs": [],
   "source": [
    "\"Discrepancy computation\"\n",
    "\n",
    "\n",
    "def compute_disc(xs, ys, xt, yt, outer_iters=100, inner_iters=1000, tol=1e-5, lr=0.1):\n",
    "    \"\"\"Given features and labels from source (xs, ys) and target (xt, yt), the function outputs the computed discrepancy between the two domains.\n",
    "    The DC programming method is used to approximate the discrepancy.\n",
    "\n",
    "    Arguments:\n",
    "      xs: source feature data of size m x d\n",
    "      ys: source label data of size m x 1\n",
    "      xt: target feature data of size n x d\n",
    "      yt: target label data of size n x 1\n",
    "      outer_iters: # outer iterations of DC programming.\n",
    "      inner_iters: # inner iterations of gradient descent for each step of DC programming.\n",
    "      tol: the objective function improvement tolerance value for the inner loop.\n",
    "      lr: learning rate for the inner loop\n",
    "    \"\"\"\n",
    "    m = xs.shape[0]\n",
    "    n = xt.shape[0]\n",
    "    d = xs.shape[1]\n",
    "\n",
    "    w = np.random.normal(size=(d, 1))\n",
    "    w /= np.linalg.norm(w)\n",
    "\n",
    "    outer_obj_val = np.inf\n",
    "\n",
    "    loss = []\n",
    "\n",
    "    for i in range(outer_iters):\n",
    "        w0 = w\n",
    "        w = np.random.normal(size=(d, 1))\n",
    "        w /= np.linalg.norm(w)\n",
    "        ypred1 = np.matmul(xs, w0)\n",
    "        ypred2 = np.matmul(xt, w0)\n",
    "\n",
    "        curr_obj_val = (\n",
    "            np.linalg.norm(ypred1 - ys) ** 2 / m - np.linalg.norm(ypred2 - yt) ** 2 / n\n",
    "        )\n",
    "        loss.append(curr_obj_val)\n",
    "        if abs(curr_obj_val - outer_obj_val) <= tol:\n",
    "            return curr_obj_val\n",
    "        outer_obj_val = curr_obj_val\n",
    "\n",
    "        \"\"\" optimize for w \"\"\"\n",
    "        inner_obj_val = np.inf\n",
    "        for j in range(inner_iters):\n",
    "            ypred = (np.matmul(xs, w) - ys).squeeze()\n",
    "            M = np.matmul(np.diag(ypred), xs)\n",
    "            grad = np.sum(M, axis=0) / m\n",
    "            grad = np.expand_dims(grad, axis=1)\n",
    "\n",
    "            ypred2 = (np.matmul(xt, w0) - yt).squeeze()\n",
    "            M = np.matmul(np.diag(ypred2), xt)\n",
    "            grad2 = np.sum(M, axis=0) / n\n",
    "            grad2 = np.expand_dims(grad2, axis=1)\n",
    "\n",
    "            grad -= grad2\n",
    "\n",
    "            w -= lr * grad\n",
    "            if np.linalg.norm(w) > 1:\n",
    "                w /= np.linalg.norm(w)\n",
    "\n",
    "            ypred1 = np.matmul(xs, w)\n",
    "            ypred2 = np.matmul(xt, w0)\n",
    "            ypred3 = np.matmul(xt, w)\n",
    "\n",
    "            curr_obj_val = (\n",
    "                0.5 * np.linalg.norm(ypred1 - ys) ** 2 / m\n",
    "                - np.sum(np.multiply(ypred2, ypred3)) / n\n",
    "            )\n",
    "            if abs(curr_obj_val - inner_obj_val) <= tol:\n",
    "                break\n",
    "            inner_obj_val = curr_obj_val\n",
    "\n",
    "    return -outer_obj_val\n",
    "\n",
    "\n",
    "def get_dbars(xs, ys, xt, yt, ms):\n",
    "    \"\"\"Given data from multiple sources and a target domain, returns a list of discrepancy values between each source and the target.\n",
    "\n",
    "    Arguments:\n",
    "      xs: source data of size m x d, where m is the total number of points from all the sources\n",
    "      ys: source data of size m x 1, where m is the total number of points from all the sources\n",
    "      xt: target feature data of size n x d\n",
    "      yt: target label data of size n x 1\n",
    "      ms: a list containing the source data and target data sizes\n",
    "    \"\"\"\n",
    "    dbars_drift = []\n",
    "    for t in range(len(ms) - 1):\n",
    "        n_t = int(np.sum(ms[:t]))  # From defintion of n_t\n",
    "        t_disc = compute_disc(xs[n_t : n_t + ms[t]], ys[n_t : n_t + ms[t]], xt, yt)\n",
    "        dbars_drift.append(abs(t_disc))\n",
    "    return dbars_drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shsllpqeRYiJ"
   },
   "source": [
    "Main alternate minimization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1677190125244,
     "user": {
      "displayName": "Pranjal Awasthi",
      "userId": "08866363694134506097"
     },
     "user_tz": 480
    },
    "id": "w0mGaKRKRJzj"
   },
   "outputs": [],
   "source": [
    "def altmin(\n",
    "    xs,\n",
    "    xt,\n",
    "    ys,\n",
    "    yt,\n",
    "    x_trgt_test,\n",
    "    y_trgt_test,\n",
    "    x_trgt_val,\n",
    "    y_trgt_val,\n",
    "    lambda_1,\n",
    "    lambda_2,\n",
    "    lambda_3,\n",
    "    dbar,\n",
    "    p0,\n",
    "    lr,\n",
    "    ms,\n",
    "    maxiters=100,\n",
    "    niters=1000,\n",
    "    q_init=None,\n",
    "    tol=1e-3,\n",
    "):\n",
    "    \"\"\"Given data from multiple sources and a target domain, the procedure runs the alternate minimization algorithm to find the best fit model for the\n",
    "    target data and reports the resulting test error.\n",
    "\n",
    "    Arguments:\n",
    "      xs: source feature data of size m x d\n",
    "      ys: source label data of size m x 1\n",
    "      xt: target feature data of size n x d\n",
    "      yt: target label data of size n x 1\n",
    "      x_trgt_test: target feature test data\n",
    "      y_trgt_test: target label test data\n",
    "      x_trgt_val: target feature validation data\n",
    "      y_trgt_val: target label validation data\n",
    "      lambda_1: regularizer term for the infinity norm of q\n",
    "      lambda_2: regularizer term for the distance from starting point (p0)\n",
    "      lambda_3: regularizer term for the squared l2 norm of q\n",
    "      dbar: a list of discrepancy values between each source and target\n",
    "      p0: the starting distribution over the data points\n",
    "      lr: the learning rate for gradient descent\n",
    "      ms: a list containing the source data and target data sizes\n",
    "      maxiters: the maximum number of iterations of alternate minimization\n",
    "      niters: the maximum number of iterations of the q optimization step\n",
    "      q_init: the initial value of q\n",
    "      tol: the objective function tolerance value for stopping criteria.\n",
    "    \"\"\"\n",
    "    print(\"Calling altmin\")\n",
    "    \"\"\" initialize q \"\"\"\n",
    "    m = xs.shape[0]\n",
    "    n = xt.shape[0]\n",
    "    x = np.vstack((xs, xt))\n",
    "    y = np.vstack((ys, yt))\n",
    "\n",
    "    if q_init is None:\n",
    "        q = np.random.uniform(size=m + n)\n",
    "        q = -np.log(q)\n",
    "        q /= np.sum(q)\n",
    "    else:\n",
    "        q = q_init\n",
    "\n",
    "    loss = []\n",
    "    prev_obj = np.inf\n",
    "    T = 0\n",
    "    best_h = []\n",
    "\n",
    "    for i in range(maxiters):\n",
    "        indices = []\n",
    "        for j in range(m + n):\n",
    "            if q[j] > 0:\n",
    "                indices.append(j)\n",
    "        clf = Ridge(alpha=lambda_1 * np.max(q), solver=\"cholesky\")\n",
    "        clf.fit(x[indices, :], y[indices], sample_weight=q[indices])\n",
    "\n",
    "        curr_h = clf.coef_\n",
    "        ypred = clf.predict(x)\n",
    "\n",
    "        \"\"\" get current objective value \"\"\"\n",
    "        curr_obj = compute_obj_val(\n",
    "            xs,\n",
    "            xt,\n",
    "            ys,\n",
    "            yt,\n",
    "            ypred,\n",
    "            q,\n",
    "            lambda_1,\n",
    "            lambda_2,\n",
    "            lambda_3,\n",
    "            p0,\n",
    "            dbar,\n",
    "            np.linalg.norm(curr_h) ** 2,\n",
    "            ms,\n",
    "        )\n",
    "        print(curr_obj)\n",
    "        if abs(curr_obj - prev_obj) <= tol:\n",
    "            best_h.append(clf)\n",
    "            T += 1\n",
    "        else:\n",
    "            T = 0\n",
    "\n",
    "        if i == maxiters - 1:\n",
    "            best_h.append(clf)\n",
    "\n",
    "        prev_obj = curr_obj\n",
    "        if T == 5:\n",
    "            break\n",
    "\n",
    "        loss.append(curr_obj)\n",
    "\n",
    "        \"\"\" Update q via gradient descent \"\"\"\n",
    "        init_lr = lr\n",
    "        best_inner_obj = curr_obj\n",
    "        best_q = q\n",
    "        iloss = ((y - ypred) ** 2).squeeze()\n",
    "        for j in range(niters):\n",
    "            grad = get_gradient(\n",
    "                xs,\n",
    "                xt,\n",
    "                q,\n",
    "                iloss,\n",
    "                lambda_1,\n",
    "                lambda_2,\n",
    "                lambda_3,\n",
    "                p0,\n",
    "                np.linalg.norm(curr_h) ** 2,\n",
    "                dbar,\n",
    "                ms,\n",
    "            )\n",
    "            grad /= np.linalg.norm(grad)\n",
    "            q = q - init_lr * grad / (j + 1)\n",
    "            q = project_simplex(q)\n",
    "            obj = compute_obj_val(\n",
    "                xs,\n",
    "                xt,\n",
    "                ys,\n",
    "                yt,\n",
    "                ypred,\n",
    "                q,\n",
    "                lambda_1,\n",
    "                lambda_2,\n",
    "                lambda_3,\n",
    "                p0,\n",
    "                dbar,\n",
    "                np.linalg.norm(curr_h) ** 2,\n",
    "                ms,\n",
    "            )\n",
    "            if obj < best_inner_obj:\n",
    "                best_inner_obj = obj\n",
    "                best_q = q\n",
    "\n",
    "        q = best_q\n",
    "    indices = []\n",
    "    for j in range(m + n):\n",
    "        if q[j] > 0:\n",
    "            indices.append(j)\n",
    "    clf = Ridge(solver=\"cholesky\")\n",
    "    clf.fit(x[indices, :], y[indices], sample_weight=q[indices])\n",
    "\n",
    "    best_h.append(clf)\n",
    "\n",
    "    \"\"\" use val data for model selection \"\"\"\n",
    "    best_error = np.inf\n",
    "    best_model = []\n",
    "    for clf in best_h:\n",
    "        ypred = clf.predict(x_trgt_val)\n",
    "        error = np.linalg.norm(y_trgt_val - ypred) / len(ypred)\n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_model = clf\n",
    "\n",
    "    ypred = best_model.predict(x_trgt_test)\n",
    "    test_error = np.linalg.norm(y_trgt_test - ypred) ** 2 / len(ypred)\n",
    "    print(\"test error = {}\".format(test_error))\n",
    "\n",
    "    return test_error, q\n",
    "\n",
    "\n",
    "def compute_obj_val(\n",
    "    xs, xt, ys, yt, ypred, q, lambda_1, lambda_2, lambda_3, p0, dbars, hnorm, ms\n",
    "):\n",
    "    \"\"\"Helper function. Computes the objective value of a given solution.\n",
    "\n",
    "    Arguments:\n",
    "      xs: source feature data of size m x d\n",
    "      ys: source label data of size m x 1\n",
    "      xt: target feature data of size n x d\n",
    "      yt: target label data of size n x 1\n",
    "      ypred: model predictions on all the data\n",
    "      q: current solution\n",
    "      lambda_1: regularizer term for the infinity norm of q\n",
    "      lambda_2: regularizer term for the distance from starting point (p0)\n",
    "      lambda_3: regularizer term for the squared l2 norm of q\n",
    "      p0: the starting distribution over the data points\n",
    "      dbars: a list of discrepancy values between each source and target\n",
    "      hnorm: the squared l2 norm of the Ridge regression predictor\n",
    "      ms: a list containing the source data and target data sizes\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "\n",
    "    y = np.vstack((ys, yt))\n",
    "    for i in range(len(q)):\n",
    "        loss += (y[i] - ypred[i]) ** 2 * q[i]\n",
    "\n",
    "    loss += lambda_3 * np.linalg.norm(q) ** 2\n",
    "\n",
    "    loss += lambda_2 * np.linalg.norm(q - p0, ord=1)\n",
    "\n",
    "    loss += lambda_1 * hnorm * np.max(q)\n",
    "\n",
    "    for t in range(len(ms) - 1):\n",
    "        # Compute q_t bar\n",
    "        n_t = int(np.sum(ms[:t]))  # From defintion of n_t\n",
    "        q_t_bar = np.sum(q[n_t : n_t + ms[t]])\n",
    "        loss += dbars[t] * q_t_bar\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_gradient(xs, xt, q, loss, lambda_1, lambda_2, lambda_3, p0, hnorm, dbars, ms):\n",
    "    \"\"\"Helper function. Computes the gradient of the objective value of a given solution with respoect to the q variable.\n",
    "\n",
    "    Arguments:\n",
    "      xs: source feature data of size m x d\n",
    "      xt: target feature data of size n x d\n",
    "      q: current solution\n",
    "      loss: model loss values on all the data\n",
    "      lambda_1: regularizer term for the infinity norm of q\n",
    "      lambda_2: regularizer term for the distance from starting point (p0)\n",
    "      lambda_3: regularizer term for the squared l2 norm of q\n",
    "      p0: the starting distribution over the data points\n",
    "      hnorm: the squared l2 norm of the Ridge regression predictor\n",
    "      dbars: a list of discrepancy values between each source and target\n",
    "      ms: a list containing the source data and target data sizes\n",
    "    \"\"\"\n",
    "    m = xs.shape[0]\n",
    "    n = xt.shape[0]\n",
    "\n",
    "    \"\"\" get loss part of gradient \"\"\"\n",
    "    loss_grad = loss\n",
    "\n",
    "    \"\"\" get dbar part of gradient \"\"\"\n",
    "    dbar_grad = np.zeros((np.sum(ms)))\n",
    "    for t in range(len(ms) - 1):\n",
    "        n_t = int(np.sum(ms[:t]))  # From defintion of n_t\n",
    "        dbar_grad[n_t : n_t + ms[t]] = dbars[t] * np.ones(ms[t])\n",
    "\n",
    "    \"\"\" get lambda_1 part of gradient \"\"\"\n",
    "    lambda_1_grad = np.zeros((m + n))\n",
    "    i1 = np.argmax(q)\n",
    "    lambda_1_grad[i1] = lambda_1 * hnorm\n",
    "\n",
    "    \"\"\" get lambda_2 part of gradient \"\"\"\n",
    "    lambda_2_grad = np.zeros((m + n))\n",
    "    lambda_2_grad = lambda_2 * np.sign(q - p0)\n",
    "\n",
    "    \"\"\" get lambda_3 part of gradient \"\"\"\n",
    "    lambda_3_grad = np.zeros((m + n))\n",
    "    lambda_3_grad = 2 * lambda_3 * q\n",
    "\n",
    "    return np.asarray(\n",
    "        loss_grad + dbar_grad + lambda_1_grad + lambda_2_grad + lambda_3_grad,\n",
    "        np.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "def project_simplex(q):\n",
    "    \"\"\"Projects a given vector onto the unit simplex.\"\"\"\n",
    "    d = q.shape[0]\n",
    "\n",
    "    qprime = -np.sort(-q)\n",
    "\n",
    "    K = 1\n",
    "    Sum = []\n",
    "    Sum.append(qprime[K - 1])\n",
    "    for i in range(2, d + 1):\n",
    "        Sum.append(Sum[-1] + qprime[i - 1])\n",
    "        if (Sum[-1] - 1) / i < qprime[i - 1]:\n",
    "            K = i\n",
    "\n",
    "    tau = (Sum[K - 1] - 1) / K\n",
    "    q = q - tau\n",
    "    for i in range(d):\n",
    "        q[i] = max(q[i], 0)\n",
    "\n",
    "    q /= np.sum(q)\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ym3xYI0FUoUM"
   },
   "source": [
    "Sample Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 129775,
     "status": "ok",
     "timestamp": 1677191072141,
     "user": {
      "displayName": "Pranjal Awasthi",
      "userId": "08866363694134506097"
     },
     "user_tz": 480
    },
    "id": "rMhcxCvKUnmy",
    "outputId": "463176dc-dde3-4c14-f5f1-b5ff91f0f7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling altmin\n",
      "[201.85463555]\n",
      "[78.6488714]\n",
      "[32.64781729]\n",
      "[19.80845977]\n",
      "[15.30804799]\n",
      "[13.66979927]\n",
      "[13.45035305]\n",
      "[13.25518565]\n",
      "[13.07684972]\n",
      "[12.91055175]\n",
      "[12.75548194]\n",
      "[12.61088836]\n",
      "[12.47605778]\n",
      "[12.35032998]\n",
      "[12.23617096]\n",
      "[12.13046561]\n",
      "[12.03190018]\n",
      "[11.94000018]\n",
      "[11.85706134]\n",
      "[11.78219484]\n",
      "[11.71229916]\n",
      "[11.6487916]\n",
      "[11.59145666]\n",
      "[11.5397616]\n",
      "[11.49364667]\n",
      "[11.45222668]\n",
      "[11.41437175]\n",
      "[11.37893591]\n",
      "[11.34618975]\n",
      "[11.31636598]\n",
      "[11.28881958]\n",
      "[11.26309509]\n",
      "[11.23910999]\n",
      "[11.21675669]\n",
      "[11.19591828]\n",
      "[11.17656088]\n",
      "[11.15900604]\n",
      "[11.14292606]\n",
      "[11.12837265]\n",
      "[11.11559337]\n",
      "[11.10444318]\n",
      "[11.09422969]\n",
      "[11.08490758]\n",
      "[11.07626063]\n",
      "[11.06856207]\n",
      "[11.06139285]\n",
      "[11.05470704]\n",
      "[11.04870922]\n",
      "[11.0432428]\n",
      "[11.03829898]\n",
      "[11.0338459]\n",
      "[11.02975421]\n",
      "[11.02591718]\n",
      "[11.02239359]\n",
      "[11.01917814]\n",
      "[11.01629949]\n",
      "[11.01372727]\n",
      "[11.01136915]\n",
      "[11.00928026]\n",
      "[11.00763046]\n",
      "[11.00617421]\n",
      "[11.00494795]\n",
      "[11.00389801]\n",
      "[11.00292169]\n",
      "[11.00207172]\n",
      "[11.00134098]\n",
      "[11.00093196]\n",
      "[11.00084562]\n",
      "test error = 0.022428921355628146\n"
     ]
    }
   ],
   "source": [
    "def generate_simulated_data(\n",
    "    d, ms, eps1, eps2, alpha=0.5, sigma=0.1, random_w=False, normalize_w_t=True\n",
    "):\n",
    "    \"\"\"Generates simulated data for D_1 to D_{T+1} Gaussian distributions.\n",
    "\n",
    "    Parameters:\n",
    "    d (int): Dimension of the data.\n",
    "    ms (int list): Sample size for each of the T+1 distributions.\n",
    "    eps1 (float list): Standard deviation for each of the T+1 distributions.\n",
    "    eps2 (float list): Measures difference between w for D_1 to D_T (has size T).\n",
    "    alpha (float): Fraction of each D_1 to D_T to leave without noise.\n",
    "    sigma (float): Standard deviation of noise to add to labels.\n",
    "\n",
    "    Returns:\n",
    "    x_d1T_train: Combined X for each of the D_1 to D_T distributions\n",
    "    y_d1T_train: Combined Y for each of the D_1 to D_T distributions\n",
    "    x_trgt_train: X train for the D_{T+1} distribution\n",
    "    y_trgt_train Y train for the D_{T+1} distribution\n",
    "    x_trgt_test: X test for the D_{T+1} distribution\n",
    "    y_trgt_test: Y test for the D_{T+1} distribution\n",
    "    x_trgt_val: X validation for the D_{T+1} distribution\n",
    "    y_trgt_val: Y validation for the D_{T+1} distribution\n",
    "    \"\"\"\n",
    "    T = len(ms) - 1\n",
    "    m1T_sum = np.sum(ms[:T])\n",
    "\n",
    "    # Will hold X for each of D_1 to D_T\n",
    "    x_d1T_train = []\n",
    "\n",
    "    for i in range(T):\n",
    "        x_d1T_train.append(\n",
    "            np.array(np.random.normal(scale=eps1[i], size=(ms[i], d)), np.float32)\n",
    "        )\n",
    "\n",
    "    x_trgt_train = np.array(\n",
    "        np.random.normal(scale=eps1[T], size=(ms[T], d)), np.float32\n",
    "    )\n",
    "\n",
    "    x_trgt_test = np.array(\n",
    "        np.random.normal(scale=eps1[T], size=(int(10 * m1T_sum), d)), np.float32\n",
    "    )\n",
    "    x_trgt_val = np.array(np.random.normal(scale=[T], size=(100, d)), np.float32)\n",
    "\n",
    "    ws = []\n",
    "    w_trgt = np.random.normal(size=(d, 1))\n",
    "    w_trgt /= np.linalg.norm(w_trgt)\n",
    "    w = np.random.normal(size=(d, 1))\n",
    "    w /= np.linalg.norm(w)\n",
    "    ws.append(w_trgt)\n",
    "    for i in range(T):\n",
    "        if random_w:\n",
    "            w = np.random.normal(size=(d, 1))\n",
    "        w_i = w_trgt + eps2[i] * w\n",
    "        if normalize_w_t:\n",
    "            w_i /= np.linalg.norm(w_i)\n",
    "        ws.insert(-1, w_i)\n",
    "\n",
    "    y_d1T_train = []\n",
    "    for i in range(T):\n",
    "        y_d1T_train.append(\n",
    "            np.matmul(x_d1T_train[i], ws[i])\n",
    "            + np.random.normal(scale=sigma, size=(ms[i], 1))\n",
    "        )\n",
    "    y_trgt_train = np.matmul(x_trgt_train, ws[T]) + np.random.normal(\n",
    "        scale=sigma, size=(ms[T], 1)\n",
    "    )\n",
    "    y_trgt_test = np.matmul(x_trgt_test, ws[T]) + np.random.normal(\n",
    "        scale=sigma, size=(10 * m1T_sum, 1)\n",
    "    )\n",
    "    y_trgt_val = np.matmul(x_trgt_val, ws[T]) + np.random.normal(\n",
    "        scale=sigma, size=(100, 1)\n",
    "    )\n",
    "\n",
    "    # introduce noise in D_1 to D_T\n",
    "    for i in range(T):\n",
    "        for j in range(int(alpha * ms[i]), ms[i]):\n",
    "            y_d1T_train[i][j] = np.linalg.norm(ws[i]) ** 2\n",
    "            x_d1T_train[i][j, :] = -100 * ws[i].squeeze()\n",
    "\n",
    "    # Flatten first T samples\n",
    "    x_d1T_train = np.array([x for s_t in x_d1T_train for x in s_t])\n",
    "    y_d1T_train = np.array([y for s_t in y_d1T_train for y in s_t])\n",
    "\n",
    "    return (\n",
    "        x_d1T_train,\n",
    "        y_d1T_train,\n",
    "        x_trgt_train,\n",
    "        y_trgt_train,\n",
    "        x_trgt_test,\n",
    "        y_trgt_test,\n",
    "        x_trgt_val,\n",
    "        y_trgt_val,\n",
    "    )\n",
    "\n",
    "\n",
    "# define data generation parameters\n",
    "num_dims = 10\n",
    "ms = [100] * 2\n",
    "standard_devs = [0.1, 10]  # eps1\n",
    "w_dists_from_trgt = [1]  # eps2\n",
    "\n",
    "# generate data\n",
    "\n",
    "xs, ys, xt, yt, x_trgt_test, y_trgt_test, x_trgt_val, y_trgt_val = (\n",
    "    generate_simulated_data(num_dims, ms, standard_devs, w_dists_from_trgt, alpha=1)\n",
    ")\n",
    "\n",
    "# compute discrepancies\n",
    "\n",
    "dbars = get_dbars(xs, ys, xt, yt, ms)\n",
    "\n",
    "\n",
    "# set up altmin parameters\n",
    "\n",
    "lambda_1 = 100\n",
    "lambda_2 = 0.1\n",
    "lambda_3 = 1000\n",
    "lr = 0.01\n",
    "\n",
    "\n",
    "m = xs.shape[0]\n",
    "n = xt.shape[0]\n",
    "p0 = np.zeros(m + n)\n",
    "p0[m : m + n] = 1.0\n",
    "p0 /= np.sum(p0)\n",
    "\n",
    "\n",
    "# call alternate minimization procedure\n",
    "test_error, q = altmin(\n",
    "    xs,\n",
    "    xt,\n",
    "    ys,\n",
    "    yt,\n",
    "    x_trgt_test,\n",
    "    y_trgt_test,\n",
    "    x_trgt_val,\n",
    "    y_trgt_val,\n",
    "    lambda_1,\n",
    "    lambda_2,\n",
    "    lambda_3,\n",
    "    dbars,\n",
    "    p0,\n",
    "    lr,\n",
    "    ms,\n",
    "    maxiters=100,\n",
    "    niters=1000,\n",
    "    q_init=None,\n",
    "    tol=1e-3,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
